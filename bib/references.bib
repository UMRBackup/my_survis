@InProceedings{pmlr-v120-yang20a,
  title = {A Theoretical Analysis of Deep {Q}-Learning},
  author = {Fan, Jianqing and Wang, Zhaoran and Xie, Yuchen and Yang, Zhuoran},
  booktitle = {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
  pages = {486--489},
  year = {2020},
  editor = {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
  volume = {120},
  series = {Proceedings of Machine Learning Research},
  month = {10--11 Jun},
  publisher =  {PMLR},
  doi = {10.48550/arXiv.1901.00137},
  url = {https://proceedings.mlr.press/v120/yang20a.html},
  keywords = {Deep Q-Learning, Markov Decision Process, Zero-Sum Markov Game},
  abstract = {Despite the great empirical success of deep reinforcement learning, its theoretical foundation is less well understood. In this work, we make the first attempt to theoretically understand the deep Q-network (DQN) algorithm (Mnih et al., 2015) from both algorithmic and statistical perspectives. In specific, we focus on the fitted Q iteration (FQI) algorithm with deep neural networks, which is a slight simplification of DQN that captures the tricks of experience replay and target network used in DQN. Under mild assumptions, we establish the algorithmic and statistical rates of convergence for the action-value functions of the iterative policy sequence obtained by FQI. In particular, the statistical error characterizes the bias and variance that arise from approximating the action-value function using deep neural network, while the algorithmic error converges to zero at a geometric rate. As a byproduct, our analysis provides justifications for the techniques of experience replay and target network, which are crucial to the empirical success of DQN. Furthermore, as a simple extension of DQN, we propose the Minimax-DQN algorithm for zero-sum Markov game with two players, which is deferred to the appendix due to space limitations.}
}

@INPROCEEDINGS{8122622,
  author={Hou, Yuenan and Liu, Lifeng and Wei, Qing and Xu, Xudong and Chen, Chunlin},
  booktitle={2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={A Novel {DDPG} Method with Prioritized Experience Replay}, 
  year={2017},
  volume={},
  number={},
  pages={316-321},
  abstract={Recently, a state-of-the-art algorithm, called deep deterministic policy gradient (DDPG), has achieved good performance in many continuous control tasks in the MuJoCo simulator. To further improve the efficiency of the experience replay mechanism in DDPG and thus speeding up the training process, in this paper, a prioritized experience replay method is proposed for the DDPG algorithm, where prioritized sampling is adopted instead of uniform sampling. The proposed DDPG with prioritized experience replay is tested with an inverted pendulum task via OpenAI Gym. The experimental results show that DDPG with prioritized experience replay can reduce the training time and improve the stability of the training process, and is less sensitive to the changes of some hyperparameters such as the size of replay buffer, minibatch and the updating rate of the target network.},
  keywords={Training;Neural networks;Machine learning;Buffer storage;Learning (artificial intelligence);Correlation;Deep Deterministic Policy Gradient;Deep Reinforcement Learning;Prioritized Experience Replay},
  doi={10.1109/SMC.2017.8122622},
  ISSN={},
  month={Oct},
  publisher={IEEE},
  url={https://ieeexplore.ieee.org/document/8122622},
}

@INPROCEEDINGS{9101333,
  author={Gao, Ziming and Gao, Yuan and Hu, Yi and Jiang, Zhengyong and Su, Jionglong},
  booktitle={2020 5th IEEE International Conference on Big Data Analytics (ICBDA)}, 
  title={Application of Deep {Q}-Network in Portfolio Management}, 
  publisher={IEEE},
  year={2020},
  volume={},
  number={},
  pages={268-275},
  abstract={Machine Learning algorithms and Neural Networks are widely applied to many different areas such as stock market prediction, facial recognition and automatic machine translation. This paper introduces a novel strategy based on the classic Deep Reinforcement Learning algorithm, Deep QNetwork, for stock market portfolio management. It is a type of deep neural network which is optimized by Q Learning. To adapt the Deep Q-Network for stock market production, we first discretize the action space so that portfolio management becomes a problem that Deep Q-Network can solve. Following this, we combine the Convolutional Neural Network and dueling Q-Net to enhance the recognition ability of the algorithm. We choose five low-relevant American stocks to test our model. It is found that the Deep Q-Network based strategy outperforms the ten other traditional strategies. The profit of Deep Q-Network algorithm is 30% more than the profit of other strategies. Moreover, the Sharpe ratio and Max Drawdown demonstrates that the risk of policy associated with Deep Q-Network is the lowest.},
  keywords={Portfolios;Tensile stress;Prediction algorithms;Neural networks;Stock markets;Learning (artificial intelligence);Machine learning;Q learning;convolutional neural network;portfolio management},
  doi={10.1109/ICBDA49040.2020.9101333},
  ISSN={},
  month={May},
  url={https://ieeexplore.ieee.org/document/9101333},
}

@ARTICLE{8966253,
  author={Xu, Yi-Han and Yang, Cheng-Cheng and Hua, Min and Zhou, Wen},
  journal={IEEE Access}, 
  publisher={IEEE},
  title={Deep Deterministic Policy Gradient ({DDPG})-Based Resource Allocation Scheme for NOMA Vehicular Communications}, 
  year={2020},
  volume={8},
  number={},
  pages={18797-18807},
  abstract={This paper investigates the resource allocation problem in vehicular communications based on multi-agent Deep Deterministic Policy Gradient (DDPG), in which each Vehicle-to-Vehicle (V2V) communication acts as agent and adopts Non-Orthogonal Multiple Access (NOMA) technology to share the frequency spectrum that pre-allocated to Vehicle-to-Infrastructure (V2I) communications. Different with conventional D2D communications, the fast varying channel condition due to the high mobility in vehicular environment causes the difficulty of collecting instantaneous Channel State Information (CSI) at base station. Meanwhile, one tremendous challenge faced by vehicular communications is how to maximize the sum-rate of V2I communications simultaneously guaranteeing the latency and reliability requirements for the transmission of safety-critical information in V2V communications. In response, we formulate the resource allocation problem as a decentralized Discrete-time and Finite-state Markov Decision Process (DFMDP), in which allocation decisions are made by multiple agents that do not have complete and global network information. Due to the complexity of the problem, we propose a DDPG algorithm which is capable of handling continuous high dimensional action spaces to find the optimal allocation strategy. Numerical results verify that each agent can effectively learn from the environment by means of the proposed DDPG algorithm to maximize the sum-rate of V2I communications while satisfying the stringent latency and reliability constraints of V2V communications.},
  keywords={Resource management;Device-to-device communication;NOMA;Vehicle-to-everything;Reliability;Quality of service;Aerospace electronics;Vehicular communications;resource allocation;deep deterministic policy gradient (DDPG);non-orthogonal multiple access (NOMA)},
  doi={10.1109/ACCESS.2020.2968595},
  ISSN={2169-3536},
  month={},
  url={https://ieeexplore.ieee.org/document/8966253},
}

@article{SUMIEA2024e30697,
  title = {Deep deterministic policy gradient algorithm: A systematic review},
  journal = {Heliyon},
  volume = {10},
  number = {9},
  pages = {e30697},
  year = {2024},
  issn = {2405-8440},
  doi = {https://doi.org/10.1016/j.heliyon.2024.e30697},
  url = {https://www.sciencedirect.com/science/article/pii/S2405844024067288},
  author={Sumiea, Ebrahim Hamid and Abdulkadir, Said Jadid and Alhussian, Hitham Seddig and Al-Selwi, Safwan Mahmood and Alqushaibi, Alawi and Ragab, Mohammed Gamal and Fati, Suliman Mohamed},
  keywords = {Deep deterministic policy gradient (DDPG), Deep reinforcement learning (DRL), Hyperparameter, Optimization},
  abstract = {Deep Reinforcement Learning (DRL) has gained significant adoption in diverse fields and applications, mainly due to its proficiency in resolving complicated decision-making problems in spaces with high-dimensional states and actions. Deep Deterministic Policy Gradient (DDPG) is a well-known DRL algorithm that adopts an actor-critic approach, synthesizing the advantages of value-based and policy-based reinforcement learning methods. The aim of this study is to provide a thorough examination of the latest developments, patterns, obstacles, and potential opportunities related to DDPG. A systematic search was conducted using relevant academic databases (Scopus, Web of Science, and ScienceDirect) to identify 85 relevant studies published in the last five years (2018-2023). We provide a comprehensive overview of the key concepts and components of DDPG, including its formulation, implementation, and training. Then, we highlight the various applications and domains of DDPG, including Autonomous Driving, Unmanned Aerial Vehicles, Resource Allocation, Communications and the Internet of Things, Robotics, and Finance. Additionally, we provide an in-depth comparison of DDPG with other DRL algorithms and traditional RL methods, highlighting its strengths and weaknesses. We believe that this review will be an essential resource for researchers, offering them valuable insights into the methods and techniques utilized in the field of DRL and DDPG.}
}

@article{van2016deep,
  title={Deep Reinforcement Learning with Double {Q}-Learning}, 
  volume={30}, 
  url={https://ojs.aaai.org/index.php/AAAI/article/view/10295}, 
  doi={10.1609/aaai.v30i1.10295}, 
  abstract={The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.}, 
  number={1}, 
  pages={2094-2100},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
  author={van Hasselt, Hado and Guez, Arthur and Silver, David}, 
  year={2016}, 
  month={Mar.},
  abstract={The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.}
}

@article{clifton2020q,
  title={{Q}-learning: Theory and applications},
  author={Clifton, Jesse and Laber, Eric},
  journal={Annual Review of Statistics and Its Application},
  issn={2326-8298},
  volume={7},
  number={1},
  pages={279-301},
  year={2020},
  publisher={Annual Reviews},
  doi={10.1146/annurev-statistics-031219-041220},
  url={https://www.annualreviews.org/doi/abs/10.1146/annurev-statistics-031219-041220},
  abstract={Q-learning, originally an incremental algorithm for estimating an optimal decision strategy in an infinite-horizon decision problem, now refers to a general class of reinforcement learning methods widely used in statistics and artificial intelligence. In the context of personalized medicine, finite-horizon Q-learning is the workhorse for estimating optimal treatment strategies, known as treatment regimes. Infinite-horizon Q-learning is also increasingly relevant in the growing field of mobile health. In computer science, Q-learning methods have achieved remarkable performance in domains such as game-playing and robotics. In this article, we (a) review the history of Q-learning in computer science and statistics, (b) formalize finite-horizon Q-learning within the potential outcomes framework and discuss the inferential difficulties for which it is infamous, and (c) review variants of infinite-horizon Q-learning and the exploration-exploitation problem, which arises in decision problems with a long time horizon. We close by discussing issues arising with the use of Q-learning in practice, including arguments for combining Q-learning with direct-search methods; sample size considerations for sequential, multiple assignment randomized trials; and possibilities for combining Q-learning with model-based methods.},
  keywords={reinforcement learning, dynamic treatment regimes, model-free, causal inference, policy search}
}

@article{Watkins1992,
  author = {Watkins, Christopher J. C. H. and Dayan, Peter},
  title = {{Q}-learning},
  journal = {Machine Learning},
  publisher = {Springer},
  year = {1992},
  volume = {8},
  number = {3},
  pages = {279-292},
  month = {5},
  abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
  keywords = {Q-learning, reinforcement learning, temporal differences, asynchronous dynamic programming},
  doi = {10.1007/BF00992698},
  url = {https://doi.org/10.1007/BF00992698},
  issn = {1573-0565}
}

@ARTICLE{8736298,
  author={Lv, Pingli and Wang, Xuesong and Cheng, Yuhu and Duan, Ziming},
  journal={IEEE Access}, 
  title={Stochastic Double Deep {Q}-Network}, 
  year={2019},
  volume={7},
  number={},
  pages={79446-79454},
  abstract={Estimation bias seriously affects the performance of reinforcement learning algorithms. The maximum operation may result in overestimation, while the double estimator operation often leads to underestimation. To eliminate the estimation bias, these two operations are combined together in our proposed algorithm named stochastic double deep Q-learning network (SDDQN), which is based on the idea of random selection. A tabular version of SDDQN is also given, named stochastic double Q-learning (SDQ). Both the SDDQN and SDQ are based on the double estimator framework. At each step, we choose to use either the maximum operation or the double estimator operation with a certain probability, which is determined by a random selection parameter. The theoretical analysis shows that there indeed exists a proper random selection parameter that makes SDDQN and SDQ unbiased. The experiments on Grid World and Atari 2600 games illustrate that our proposed algorithms can balance the estimation bias effectively and improve performance.},
  keywords={Estimation;Linear programming;Reinforcement learning;Games;Neural networks;Data mining;Estimation bias;deep reinforcement learning;maximum operation;double estimator operation;stochastic combination},
  doi={10.1109/ACCESS.2019.2922706},
  ISSN={2169-3536},
  url={https://ieeexplore.ieee.org/document/8736298},
  publisher={IEEE}
}

@inproceedings{3387199,
  author = {Dankwa, Stephen and Zheng, Wenfeng},
  title = {Twin-Delayed {DDPG}: A Deep Reinforcement Learning Technique to Model a Continuous Movement of an Intelligent Robot Agent},
  year = {2020},
  isbn = {9781450376259},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3387168.3387199},
  doi = {10.1145/3387168.3387199},
  abstract = {In this current research, Twin-Delayed {DDPG} (TD3) algorithm has been used to solve the most challenging virtual Artificial Intelligence application by training a 4-ant-legged robot as an Intelligent Agent to run across a field. Twin-Delayed DDPG (TD3) is an incredibly smart AI model of a Deep Reinforcement Learning which combines the state-of-the-art methods in Artificial Intelligence. These includes Policy gradient, Actor-Critics, and continuous Double Deep Q-Learning. These Deep Reinforcement Learning approaches trained an Intelligent agent to interact with an environment with automatic feature engineering, that is, necessitating minimal domain knowledge. For the implementation of the TD3, we used a two-layer feedforward neural network of 400 and 300 hidden nodes respectively, with Rectified Linear Units (ReLU) as an activation function between each layer for both the Actor and Critics. We, then added a final tanh unit after the output of the Actor. The Critic receives both the state and action as input to the first layer. Both the network parameters were updated using Adam optimizer. The idea behind the Twin-Delayed DDPG (TD3) is to reduce overestimation bias in Deep Q-Learning with discrete actions which are ineffective in an Actor-Critic domain setting. Based on the Maximum Average Reward over the evaluation time-step, our model achieved an approximate maximum of 2364. Therefore, we can truly say that, TD3 has obviously improved on both the learning speed and performance of the Deep Deterministic Policy Gradient (DDPG) in a challenging environment in a continuous control domain.},
  booktitle = {Proceedings of the 3rd International Conference on Vision, Image and Signal Processing},
  articleno = {66},
  pages = {1-5},
  keywords = {Twin-Delayed Deep Deterministic Policy Gradient, Deep Reinforcement Learning, Artificial Intelligence, Actor-Critic},
  location = {Vancouver, BC, Canada},
  series = {ICVISP 2019}
}